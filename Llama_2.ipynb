{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRKuyShlph1eYqbxoe4I/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shiveshrane/Research_paper_implementations/blob/main/Llama_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n"
      ],
      "metadata": {
        "id": "-9AoFDrvzXhc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2V2OP1rJAiJs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "oHpaEdMFtP7z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precompute RoPE"
      ],
      "metadata": {
        "id": "T8GmkU6k8_TK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensorflow version"
      ],
      "metadata": {
        "id": "a9pCVrCu_8Hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryEmbeddingsTF(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim, max_seq_len, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.dim=dim\n",
        "    self.max_seq_len=max_seq_len\n",
        "    self.theta=tf.pow(10000, -2 * tf.range(0, dim, 2, dtype=tf.float32) / tf.cast(dim, tf.float32))\n",
        "    positions=tf.range(max_seq_len, dtype=tf.float32)\n",
        "    positions=tf.reshape(positions, (-1,1))\n",
        "    self.cos=tf.cos(positions * self.theta)\n",
        "    self.sin=tf.sin(positions * self.theta)\n",
        "\n",
        "  def call(self, x, start_pos):\n",
        "    batch, seq_len, n_heads, head_dim=x.shape\n",
        "    x_reshaped=tf.reshape(x, (batch, seq_len, n_heads, head_dim//2, 2)) # Reshape to (batch, seq_len, n_heads, head_dim//2, 2)\n",
        "    start_pos=tf.convert_to_tensor(start_pos, dtype=tf.int32)\n",
        "    cos=tf.slice(self.cos, [start_pos, 0], [seq_len, head_dim//2]) # Slice cos based on head_dim//2\n",
        "    sin=tf.slice(self.sin, [start_pos, 0], [seq_len, head_dim//2]) # Slice sin based on head_dim//2\n",
        "    cos=tf.expand_dims(tf.expand_dims(cos, 0), 2) # Expand dimensions to match x_reshaped\n",
        "    sin=tf.expand_dims(tf.expand_dims(sin, 0), 2) # Expand dimensions to match x_reshaped\n",
        "    #print(self.cos, self.sin)\n",
        "    x0=x_reshaped[..., 0]\n",
        "    x1=x_reshaped[..., 1]\n",
        "    x_rot=tf.stack([x0*cos - x1*sin, x0*sin + x1*cos], axis=-1)\n",
        "    return tf.reshape(x_rot, (batch, seq_len, n_heads, head_dim)) # Reshape back to original shape"
      ],
      "metadata": {
        "id": "gABCpTT_tLgI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize layer\n",
        "rotary_emb = RotaryEmbeddingsTF(dim=128, max_seq_len=2048)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Create dummy input (batch_size=2, seq_len=10, n_heads=8, head_dim=128)\n",
        "x = tf.random.normal((2, 10, 8, 128))\n",
        "\n",
        "# Apply rotary embeddings\n",
        "output = rotary_emb(x, start_pos=0)\n",
        "print(output.shape)  # (2, 10, 8, 128)"
      ],
      "metadata": {
        "id": "CJ5OmKyv9HIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257cf17a-672f-4620-cd98-36e34a1b8c56"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 10, 8, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pytorch version"
      ],
      "metadata": {
        "id": "foIIJS2wABgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RotaryEmbeddings(nn.Module):\n",
        "    def __init__(self, dim, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Precompute theta for dimension pairs\n",
        "        theta = 10000 ** (-2 * torch.arange(0, dim//2, dtype=torch.float32) / dim)\n",
        "        self.register_buffer(\"theta\", theta)  # (dim//2,)\n",
        "\n",
        "        # Precompute sin/cos for all positions\n",
        "        positions = torch.arange(max_seq_len, dtype=torch.float32).unsqueeze(1)  # (max_seq_len, 1)\n",
        "        angles = positions * self.theta.unsqueeze(0)  # (max_seq_len, dim//2)\n",
        "        self.register_buffer(\"cos\", torch.cos(angles))  # (max_seq_len, dim//2)\n",
        "        self.register_buffer(\"sin\", torch.sin(angles))  # (max_seq_len, dim//2)\n",
        "\n",
        "    def forward(self, x, start_pos=0):\n",
        "        batch, seq_len, n_heads, head_dim = x.shape\n",
        "        x = x.view(batch, seq_len, n_heads, head_dim//2, 2)  # (B, T, H, D//2, 2)\n",
        "\n",
        "        # Slice cos/sin and add dimensions for broadcasting\n",
        "        cos = self.cos[start_pos:start_pos+seq_len]  # (T_slice, D//2)\n",
        "        sin = self.sin[start_pos:start_pos+seq_len]  # (T_slice, D//2)\n",
        "        cos = cos.unsqueeze(0).unsqueeze(2)  # (1, T_slice, 1, D//2)\n",
        "        sin = sin.unsqueeze(0).unsqueeze(2)  # (1, T_slice, 1, D//2)\n",
        "\n",
        "        # Apply rotation: [x0, x1] â†’ [x0*cos - x1*sin, x0*sin + x1*cos]\n",
        "        x_rot = torch.stack([\n",
        "            x[..., 0] * cos - x[..., 1] * sin,\n",
        "            x[..., 0] * sin + x[..., 1] * cos\n",
        "        ], dim=-1)  # (B, T, H, D//2, 2)\n",
        "\n",
        "        return x_rot.view(batch, seq_len, n_heads, head_dim)  # (B, T, H, D)"
      ],
      "metadata": {
        "id": "sEQd0IjGeukk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "x=torch.randn(2, 10, 8, 128)\n",
        "rotary_emb=RotaryEmbeddings(dim=128, max_seq_len=2048)\n",
        "output=rotary_emb(x, start_pos=0)\n",
        "output.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPa8lSNVMpKe",
        "outputId": "4e25c97f-90a0-4080-c6d4-155a540c11c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 8, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import torch\n",
        "def apply_rotary_embedding_torch(x: torch.Tensor, freqs_complex: torch.Tensor, device: str):\n",
        "    # Separate the last dimension pairs of two values, representing the real and imaginary parts of the complex number\n",
        "    # Two consecutive values will become a single complex number\n",
        "    # (B, Seq_Len, H, Head_Dim) -> (B, Seq_Len, H, Head_Dim/2)\n",
        "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "    # Reshape the freqs_complex tensor to match the shape of the x_complex tensor. So we need to add the batch dimension and the head dimension\n",
        "    # (Seq_Len, Head_Dim/2) --> (1, Seq_Len, 1, Head_Dim/2)\n",
        "    freqs_complex = freqs_complex.unsqueeze(0).unsqueeze(2)\n",
        "    # Multiply each complex number in the x_complex tensor by the corresponding complex number in the freqs_complex tensor\n",
        "    # Which results in the rotation of the complex number as shown in the Figure 1 of the paper\n",
        "    # (B, Seq_Len, H, Head_Dim/2) * (1, Seq_Len, 1, Head_Dim/2) = (B, Seq_Len, H, Head_Dim/2)\n",
        "    x_rotated = x_complex * freqs_complex\n",
        "    # Convert the complex number back to the real number\n",
        "    # (B, Seq_Len, H, Head_Dim/2) -> (B, Seq_Len, H, Head_Dim/2, 2)\n",
        "    x_out = torch.view_as_real(x_rotated)\n",
        "    # (B, Seq_Len, H, Head_Dim/2, 2) -> (B, Seq_Len, H, Head_Dim)\n",
        "    x_out = x_out.reshape(*x.shape)\n",
        "    return x_out.type_as(x).to(device)"
      ],
      "metadata": {
        "id": "dlKrKKGqmx8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSNorm"
      ],
      "metadata": {
        "id": "QXUovqPRcBqq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSNorm in pytorch"
      ],
      "metadata": {
        "id": "8MAYzAH1cETG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMS_Norm_Torch(nn.Module):\n",
        "  def __init__(self, dim, eps=1e-6 ):\n",
        "    super().__init__()\n",
        "    self.eps=eps\n",
        "    ## Gamma parameter\n",
        "    self.weight=nn.Parameter(torch.ones(dim))\n",
        "\n",
        "  def norm(self, x):\n",
        "    # x.shape= (B,Seq_len, Dim)\n",
        "    rms_val=torch.sqrt(x.pow(2).mean(-1, keepdims=True)+self.eps)\n",
        "    return x/rms_val\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.weight*self.norm(x)"
      ],
      "metadata": {
        "id": "2rUjUhgCcGjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test RMS\n",
        "x=torch.ones(1,10,128)\n",
        "rms=RMS_Norm_Torch(128)\n",
        "rms(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpONRL1V0pSZ",
        "outputId": "1e269bad-1518-437b-b9dc-5c9085cb84b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         ...,\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
              "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RMSNorm in Tensorflow"
      ],
      "metadata": {
        "id": "h5w-ydyNcG8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMS_Norm_TF(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim, eps=1e-6, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.eps=eps\n",
        "    self.weight=self.add_weight(\n",
        "        name=\"rms_weight\",\n",
        "        shape=(dim,),\n",
        "        initializer=\"ones\",\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "  def rms_calc(self, x):\n",
        "    rms_val=tf.sqrt(tf.reduce_mean(tf.square(x), axis=-1, keepdims=True)+self.eps)\n",
        "    return x/rms_val\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.weight*self.rms_calc(x)"
      ],
      "metadata": {
        "id": "zAVvUs3ccJEG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test\n",
        "x=tf.ones((1,10,128))\n",
        "rms=RMS_Norm_TF(128)\n",
        "rms(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM_M1Gtx0sFE",
        "outputId": "355de350-19d3-4e7a-a41f-bcedde014928"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10, 128), dtype=float32, numpy=\n",
              "array([[[0.9999995, 0.9999995, 0.9999995, ..., 0.9999995, 0.9999995,\n",
              "         0.9999995],\n",
              "        [0.9999995, 0.9999995, 0.9999995, ..., 0.9999995, 0.9999995,\n",
              "         0.9999995],\n",
              "        [0.9999995, 0.9999995, 0.9999995, ..., 0.9999995, 0.9999995,\n",
              "         0.9999995],\n",
              "        ...,\n",
              "        [0.9999995, 0.9999995, 0.9999995, ..., 0.9999995, 0.9999995,\n",
              "         0.9999995],\n",
              "        [0.9999995, 0.9999995, 0.9999995, ..., 0.9999995, 0.9999995,\n",
              "         0.9999995],\n",
              "        [0.9999995, 0.9999995, 0.9999995, ..., 0.9999995, 0.9999995,\n",
              "         0.9999995]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouped Query Attention"
      ],
      "metadata": {
        "id": "GqS_1iV4cKBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GQA in Pytorch"
      ],
      "metadata": {
        "id": "geXrgbxrcNDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def repeat_kv(x, n_rep):\n",
        "  batch_size, seq_len, n_kv_heads, head_dim=x.shape\n",
        "  if n_rep==1:\n",
        "    return x\n",
        "  else:\n",
        "    return(\n",
        "        # (B, Seq_len, KV_heads, Head_dim)\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(batch_size, seq_len, n_kv_heads*n_rep, head_dim)\n",
        "    )"
      ],
      "metadata": {
        "id": "nK0LOYo9gVKd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "RQTwSzX_c1jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionTorch(nn.Module):\n",
        "  def __init__(self, n_heads, dim, max_seq_len, max_batch_size, use_cache=True, n_kv_heads=None):\n",
        "    super().__init__()\n",
        "\n",
        "    ## No of heads for Query\n",
        "    self.n_heads_q=n_heads\n",
        "\n",
        "    ## No of heads for Key and Value (same as query if kv heads=None)\n",
        "    self.n_kv_heads= n_heads if n_kv_heads is None else n_kv_heads\n",
        "\n",
        "    ## No of times key and value needs to be repeated to match the head of the queries\n",
        "    self.n_rep=self.n_heads_q//self.n_kv_heads\n",
        "\n",
        "    # indicates the dimension of each head\n",
        "    self.head_dim=dim//self.n_heads_q\n",
        "\n",
        "    self.wq=nn.Linear(dim, self.n_heads_q*self.head_dim, bias=False)\n",
        "    self.wk=nn.Linear(dim, self.n_kv_heads*self.head_dim, bias=False)\n",
        "    self.wv=nn.Linear(dim, self.n_kv_heads*self.head_dim, bias=False)\n",
        "    self.wo=nn.Linear(self.n_heads_q*self.head_dim, dim, bias=False)\n",
        "\n",
        "\n",
        "    self.inference=use_cache\n",
        "\n",
        "    if self.inference:\n",
        "      self.cache_k=torch.zeros((max_batch_size, max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "      self.cache_v=torch.zeros((max_batch_size, max_seq_len, self.n_kv_heads, self.head_dim))\n",
        "\n",
        "    self.rotary=RotaryEmbeddings(self.head_dim, max_seq_len)\n",
        "\n",
        "\n",
        "  def forward(self, x, start_pos):\n",
        "    batch_size, seq_len, _=x.shape\n",
        "\n",
        "    # (B,1, Dim)--> (B,1, H_Q*Head_dim)\n",
        "    xq=self.wq(x)\n",
        "\n",
        "    # (B,1, Dim)--> (B,1, H_kv*Head_dim)\n",
        "    xk=self.wk(x)\n",
        "\n",
        "    ## (B,1, Dim)--> (B,1, H_KV*Head_dim)\n",
        "    xv=self.wv(x)\n",
        "\n",
        "    # (B,1, HQ*Head_dim)--> (B,1, HQ, head_dim)\n",
        "    xq=xq.view(batch_size, seq_len, self.n_heads_q, self.head_dim)\n",
        "\n",
        "    # (B,1, HQ*Head_dim)--> (B,1, HKV, head_dim)\n",
        "    xk=xk.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "    # (B,1, HQ*Head_dim)--> (B,1, HkV, head_dim)\n",
        "    xv=xv.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "\n",
        "      # Does not change shape\n",
        "   # rotary_matrix = self.rotary.init_matrix(seq_len)\n",
        "    #xq = torch.einsum(\"bihd,ijd->bihj\", xq, rotary_matrix)\n",
        "    #k = torch.einsum(\"bihd,ijd->bihj\", xk, rotary_matrix)\n",
        "\n",
        "    xq=self.rotary(xq, start_pos)\n",
        "    xk=self.rotary(xk, start_pos)\n",
        "    #print(xq.shape\n",
        "    ## Einsum is same as matrix mult\n",
        "    \"\"\"\n",
        "    xq= xq @ rotatory_matrix.transpose(perm=(0,2,1)) # Gives shape (B, seq_len, heads, head_dim) (since 1 and 2 of matrix is head_dim itself)\n",
        "    xk= xk @ rotatory_matrix.transpose(perm=(0,2,1))\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "      # Replace the entry in the cache for this token\n",
        "    if self.inference:\n",
        "      self.cache_k[:batch_size, start_pos:start_pos+seq_len]=xk\n",
        "      self.cache_v[:batch_size, start_pos:start_pos+seq_len]=xv\n",
        "\n",
        "     #(B, Seq_len_KV,h_kv, head_dim)\n",
        "     # Retrieve all values.\n",
        "      keys=self.cache_k[:batch_size, 0:start_pos+seq_len]\n",
        "      values=self.cache_v[:batch_size, 0:start_pos+seq_len]\n",
        "\n",
        "\n",
        "    # Repeat the heads of K and V to reach the number of heads of the queries\n",
        "\n",
        "    else:\n",
        "      keys=xk\n",
        "      values=xv\n",
        "\n",
        "    keys=repeat_kv(keys, self.n_rep)\n",
        "    values=repeat_kv(values, self.n_rep)\n",
        "\n",
        "    # (B, 1, H_Q, Head_dim)--> (B, H_Q, 1, head_dim)\n",
        "    xq=xq.transpose(1,2)\n",
        "    keys=keys.transpose(1,2)\n",
        "    values=values.transpose(1,2)\n",
        "\n",
        "\n",
        "    # (B, H_Q, 1, Head_Dim) @(B, H_kv, Head_dim, seq_len_kv)==> (B, HQ, 1, seq_len_kv)\n",
        "    scores=torch.matmul(xq, keys.transpose(2,3))/math.sqrt(self.head_dim)\n",
        "\n",
        "    if self.inference==False:\n",
        "      mask=torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool, device=x.device), diagonal=1)\n",
        "      scores.masked_fill_(mask, -np.inf)\n",
        "\n",
        "\n",
        "    scores=torch.softmax(scores, dim=-1).type_as(xq)\n",
        "    #print(scores)\n",
        "\n",
        "    # (B, hq,1,seq_len_kv)@ (B,H_kv, seq_len_kv, head_dim)--> (B,Hq, 1, head_dim)\n",
        "    output=torch.matmul(scores, values)\n",
        "\n",
        "\n",
        "\n",
        "    #(B, HQ, 1, Head_dim)--> (B, 1,HQ, Head_dim)\n",
        "    output=(output.transpose(1,2).contiguous().view(batch_size, seq_len, -1))\n",
        "    return self.wo(output) #(B, 1, Dim)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PiJNb1xWcP5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.ones(1,10,128)\n",
        "attn=SelfAttentionTorch(n_heads=16, dim=128, max_seq_len=10, max_batch_size=1, use_cache=False)\n",
        "val=attn(x, 0)\n",
        "val, val.shape"
      ],
      "metadata": {
        "id": "Qa2P6E3ulK09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338d8277-dcd7-401b-845d-98c037745899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.0036, -0.0640,  0.0164,  ..., -0.4840, -0.5122, -0.0622],\n",
              "          [ 0.0036, -0.0640,  0.0164,  ..., -0.4840, -0.5122, -0.0622],\n",
              "          [ 0.0036, -0.0640,  0.0164,  ..., -0.4840, -0.5122, -0.0622],\n",
              "          ...,\n",
              "          [ 0.0036, -0.0640,  0.0164,  ..., -0.4840, -0.5122, -0.0622],\n",
              "          [ 0.0036, -0.0640,  0.0164,  ..., -0.4840, -0.5122, -0.0622],\n",
              "          [ 0.0036, -0.0640,  0.0164,  ..., -0.4840, -0.5122, -0.0622]]],\n",
              "        grad_fn=<UnsafeViewBackward0>),\n",
              " torch.Size([1, 10, 128]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=torch.ones(1,10,64)\n",
        "attn=SelfAttentionTorch(n_heads=16, dim=64, max_seq_len=10, max_batch_size=1, use_cache=False)\n",
        "attn(x, 0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgisQ1by1XR5",
        "outputId": "3d57a435-9c67-485d-c048-acb706882851",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GQA in Tensorflow"
      ],
      "metadata": {
        "id": "a1-_bNwccQJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionTF(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_heads, dim, max_seq_len, max_batch_size, use_cache=False, n_kv_heads=None):\n",
        "    super().__init__()\n",
        "    self.n_heads_q=n_heads\n",
        "    self.n_kv_heads= n_heads if n_kv_heads is None else n_kv_heads\n",
        "    self.n_rep=self.n_heads_q//self.n_kv_heads\n",
        "    self.head_dim=dim//self.n_heads_q\n",
        "    self.wq=tf.keras.layers.Dense(self.n_heads_q*self.head_dim, use_bias=False)\n",
        "    self.wk=tf.keras.layers.Dense(self.n_kv_heads*self.head_dim, use_bias=False)\n",
        "    self.wv=tf.keras.layers.Dense(self.n_kv_heads*self.head_dim, use_bias=False)\n",
        "    self.wo=tf.keras.layers.Dense(dim, use_bias=False)\n",
        "    #self.freqs=precompute_theta_pos_frequencies(self.head_dim, max_seq_len)//For using the function method\n",
        "\n",
        "    self.inference=use_cache\n",
        "    #if self.inference:\n",
        "     # self.cache_k=tf.Variable(tf.zeros((max_batch_size, max_seq_len, self.n_kv_heads, self.head_dim)))\n",
        "      #self.cache_v=tf.Variable(tf.zeros((max_batch_size, max_seq_len, self.n_kv_heads, self.head_dim)))\n",
        "    if self.inference:\n",
        "            self.cache_k = self.add_weight(\n",
        "                name=\"cache_k\",\n",
        "                shape=(max_batch_size, max_seq_len, self.n_kv_heads, self.head_dim),\n",
        "                initializer=\"zeros\",\n",
        "                trainable=False,\n",
        "            )\n",
        "            self.cache_v = self.add_weight(\n",
        "               name= \"cache_v\",\n",
        "                shape=(max_batch_size, max_seq_len, self.n_kv_heads, self.head_dim),\n",
        "                initializer=\"zeros\",\n",
        "                trainable=False,\n",
        "            )\n",
        "    self.rotary=RotaryEmbeddingsTF(dim, max_seq_len)\n",
        "\n",
        "  def call(self, x, start_pos):\n",
        "      batch_size, seq_len, _=x.shape\n",
        "      xq=self.wq(x)\n",
        "      xk=self.wk(x)\n",
        "      xv=self.wv(x)\n",
        "\n",
        "      xq=tf.reshape(xq, shape=(batch_size, seq_len, self.n_heads_q, self.head_dim))\n",
        "      xk=tf.reshape(xk, shape=(batch_size, seq_len, self.n_kv_heads, self.head_dim))\n",
        "      xv=tf.reshape(xv, shape=(batch_size, seq_len, self.n_kv_heads, self.head_dim))\n",
        "\n",
        "      xq=self.rotary(xq, start_pos=start_pos)\n",
        "      xk=self.rotary(xk, start_pos=start_pos)\n",
        "      #xq=apply_rotary_embeddings(xq, self.freqs)\n",
        "      #xk=apply_rotary_embeddings(xk, self.freqs)\n",
        "      if self.inference:\n",
        "        self.cache_k[:batch_size, start_pos:start_pos+seq_len].assign(xk)\n",
        "        self.cache_v[:batch_size, start_pos:start_pos+seq_len].assign(xv)\n",
        "        keys=self.cache_k[:batch_size, :start_pos+seq_len]\n",
        "        values=self.cache_v[:batch_size, :start_pos+seq_len]\n",
        "      else:\n",
        "        keys=xk\n",
        "        values=xv\n",
        "\n",
        "      keys=tf.repeat(keys, repeats=self.n_rep, axis=2)\n",
        "      values=tf.repeat(values, repeats=self.n_rep, axis=2)\n",
        "      #print(self.n_rep,keys.shape, values.shape)\n",
        "\n",
        "      xq=tf.transpose(xq, perm=(0,2,1,3))\n",
        "      keys=tf.transpose(keys, perm=(0,2,1,3))\n",
        "      values=tf.transpose(values, perm=(0,2,1,3))\n",
        "      scores=tf.matmul(xq, keys, transpose_b=True)/math.sqrt(self.head_dim)\n",
        "      if self.inference==False:\n",
        "        mask=tf.linalg.band_part(tf.ones((seq_len, seq_len), dtype=tf.bool), -1, 0)\n",
        "        mask=tf.reshape(mask, (1,1,seq_len, seq_len))\n",
        "        #print(mask)\n",
        "        scores= tf.where(mask, scores, -np.inf)\n",
        "\n",
        "      attn=tf.nn.softmax(scores, axis=-1)\n",
        "      output=tf.matmul(attn, values)\n",
        "      output=tf.transpose(output, perm=(0,2,1,3))\n",
        "      output=tf.reshape(output, shape=(batch_size, seq_len, self.n_heads_q*self.head_dim))\n",
        "      return self.wo(output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bh3Lwq6M-FPF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_self_attention_layer():\n",
        "    batch_size = 1\n",
        "    seq_len = 10\n",
        "    model_dim = 64\n",
        "    n_heads = 4\n",
        "    max_seq_len = 16\n",
        "    max_batch_size = 4\n",
        "    n_kv_heads = 2\n",
        "\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    x = tf.random.uniform((batch_size, seq_len, model_dim))\n",
        "    print(\"Input shape:\", x.shape)\n",
        "\n",
        "    print(\"\\n--- Testing SelfAttentionTF in Training Mode (no caching) ---\")\n",
        "    sa_layer_train = SelfAttentionTF(n_heads, model_dim, max_seq_len, max_batch_size, n_kv_heads=n_kv_heads, use_cache=False)\n",
        "    output_train = sa_layer_train(x, start_pos=0)  # Pass start_pos as a keyword argument\n",
        "    print(\"Training mode output shape:\", output_train.shape)\n",
        "    output=RMS_Norm_TF(model_dim)(output_train)\n",
        "    print(output)\n",
        "   # print(\"Training mode output:\", output_train.numpy())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    test_self_attention_layer()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xE0tOCDo33l",
        "outputId": "84ccacb2-4289-463f-8a62-ed593d471b08",
        "collapsed": true
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (1, 10, 64)\n",
            "\n",
            "--- Testing SelfAttentionTF in Training Mode (no caching) ---\n",
            "Training mode output shape: (1, 10, 64)\n",
            "tf.Tensor(\n",
            "[[[-1.32251644e+00  3.17307174e-01 -5.68085276e-02  2.32912445e+00\n",
            "    1.05909801e+00 -2.28188252e+00 -1.40880480e-01 -7.88160622e-01\n",
            "   -8.72630417e-01  1.38920736e+00 -1.93686873e-01  7.01617301e-01\n",
            "    1.12299621e+00  7.74149776e-01 -7.75494814e-01  9.81954515e-01\n",
            "    3.59548837e-01 -1.19232142e+00  2.78841078e-01 -3.78031582e-01\n",
            "    9.25838947e-02  7.48536170e-01 -4.94152397e-01 -1.48785448e+00\n",
            "   -7.93696165e-01  2.70074129e-01  4.26028550e-01  1.06547391e+00\n",
            "    1.61096418e+00 -1.34831417e+00 -3.11176538e-01 -1.47922993e-01\n",
            "   -7.99997866e-01 -1.42394245e+00 -3.36356193e-01  2.28861070e+00\n",
            "   -1.55109298e+00 -1.37776339e+00 -1.17986917e+00  2.78266221e-01\n",
            "   -8.96787703e-01 -1.68970430e+00 -7.40677476e-01 -1.57817125e+00\n",
            "    1.00614083e+00  3.25292677e-01  2.98547834e-01  8.03892612e-01\n",
            "   -1.54277131e-01 -3.02864194e-01 -1.03238754e-01 -1.14179756e-02\n",
            "   -2.36216951e-02  2.30225652e-01  6.55665815e-01 -7.50365734e-01\n",
            "   -1.26821923e+00 -1.83747458e+00 -5.77355064e-02  1.36013317e+00\n",
            "    3.18669766e-01  7.52513349e-01  4.57706362e-01  7.23145664e-01]\n",
            "  [-9.77554679e-01  3.17227483e-01 -2.50744730e-01  1.90489769e+00\n",
            "    1.29589474e+00 -2.19717956e+00  6.47883192e-02 -5.27996957e-01\n",
            "   -7.05293417e-01  1.45325208e+00  6.04637623e-01  4.14045691e-01\n",
            "    4.53452796e-01  1.01576829e+00 -4.17924523e-01  4.18670684e-01\n",
            "    3.73910144e-02 -1.53783619e+00  3.68389636e-01 -7.99534500e-01\n",
            "    6.13181591e-01  1.01039791e+00 -9.85967219e-01 -1.51801991e+00\n",
            "   -8.67235243e-01  5.95211327e-01  6.44904613e-01  1.04395497e+00\n",
            "    1.01198709e+00 -1.62385821e+00 -3.65299761e-01 -1.09652236e-01\n",
            "   -1.41435850e+00 -1.28310382e+00 -1.65030500e-03  2.13641167e+00\n",
            "   -1.60010803e+00 -1.84951568e+00 -6.32716715e-01  5.90395927e-01\n",
            "   -9.56892967e-01 -1.47171867e+00 -4.59639132e-01 -1.93694615e+00\n",
            "    1.23079360e+00 -1.25209078e-01  3.23818237e-01  9.10902321e-01\n",
            "   -5.18358886e-01 -7.90191352e-01 -2.67413668e-02 -1.93371594e-01\n",
            "    3.08175236e-01  4.93770331e-01  4.25462067e-01 -5.65750957e-01\n",
            "   -1.35076630e+00 -1.41458535e+00 -5.85273027e-01  1.00930786e+00\n",
            "    8.51254165e-01  8.39013398e-01  3.36684197e-01  7.37847209e-01]\n",
            "  [-9.64615345e-01  4.17649686e-01 -2.11438403e-01  1.62797439e+00\n",
            "    1.53159440e+00 -1.69328201e+00  3.67743075e-01 -3.53809111e-02\n",
            "   -1.12242544e+00  1.13071752e+00  3.12311530e-01  3.05904478e-01\n",
            "    5.81422783e-02  1.06728542e+00 -3.72595668e-01  3.81758809e-01\n",
            "   -2.95867115e-01 -1.77850986e+00  7.32065976e-01 -9.09215748e-01\n",
            "    3.43783408e-01  1.46118164e+00 -1.13600194e+00 -1.59439516e+00\n",
            "   -8.69225502e-01  7.69563615e-01  5.77393711e-01  9.66458619e-01\n",
            "    5.09312570e-01 -2.10980582e+00 -1.73650190e-01 -1.82879195e-01\n",
            "   -1.03276026e+00 -1.37755406e+00  4.27236617e-01  2.13428593e+00\n",
            "   -1.64768124e+00 -1.93870139e+00 -2.62536854e-01  6.90689802e-01\n",
            "   -9.06524122e-01 -1.11153173e+00 -4.81509060e-01 -2.20262194e+00\n",
            "    1.24557436e+00 -2.27188602e-01  2.72794306e-01  8.83970380e-01\n",
            "   -9.06893164e-02 -1.30270100e+00 -9.81443003e-02 -4.56669539e-01\n",
            "    5.07797860e-02  6.63223505e-01  7.14137912e-01 -3.23537976e-01\n",
            "   -1.11211431e+00 -1.28420961e+00 -5.85244477e-01  9.12346721e-01\n",
            "    2.68620461e-01  9.07150745e-01  1.00062221e-01  5.81266582e-01]\n",
            "  [-1.13672519e+00  3.95094067e-01 -9.50710997e-02  1.49686551e+00\n",
            "    1.48762619e+00 -1.70589888e+00  3.99076194e-01  1.22977838e-01\n",
            "   -1.18569481e+00  1.08849287e+00  1.81524456e-01  5.15482843e-01\n",
            "    3.14070433e-01  1.01033545e+00 -6.13444783e-02  3.11652601e-01\n",
            "   -1.26846358e-01 -1.88464355e+00  6.62421227e-01 -7.11273909e-01\n",
            "    3.75900626e-01  1.51781428e+00 -1.13838387e+00 -1.39430213e+00\n",
            "   -1.05553401e+00  9.23485577e-01  5.30548871e-01  1.02021027e+00\n",
            "    2.13765591e-01 -2.16324329e+00 -1.66442484e-01 -1.95824429e-01\n",
            "   -1.10479832e+00 -1.30292571e+00  4.05286700e-01  2.15227652e+00\n",
            "   -1.46485221e+00 -2.02677274e+00 -7.36044422e-02  7.87171900e-01\n",
            "   -8.30098033e-01 -1.05584490e+00 -5.41016757e-01 -2.09960413e+00\n",
            "    1.35847831e+00 -4.05308425e-01  3.93733531e-01  6.53704107e-01\n",
            "    5.47971427e-02 -1.38914096e+00 -2.56563187e-01 -4.58349079e-01\n",
            "    1.84401181e-02  6.45791650e-01  9.08681571e-01 -1.54170230e-01\n",
            "   -1.15362215e+00 -1.29593778e+00 -6.36226058e-01  7.32786953e-01\n",
            "    4.23775882e-01  1.02101076e+00 -9.26877633e-02  5.17429054e-01]\n",
            "  [-1.30937254e+00  6.59867153e-02 -1.03579678e-01  1.58494890e+00\n",
            "    1.31967556e+00 -1.82081246e+00  4.58297223e-01 -2.79099531e-02\n",
            "   -1.12335169e+00  1.14093804e+00  1.07407615e-01  8.87929142e-01\n",
            "    3.88696104e-01  9.49205160e-01  3.90449986e-02  3.32929313e-01\n",
            "   -3.90847400e-02 -1.73265946e+00  4.99643981e-01 -5.20060778e-01\n",
            "    2.58532941e-01  1.37027729e+00 -9.48824227e-01 -1.41867793e+00\n",
            "   -1.12938523e+00  8.81187618e-01  7.78811395e-01  9.12277937e-01\n",
            "    4.89719033e-01 -1.69760203e+00 -2.31380463e-01 -6.97574690e-02\n",
            "   -1.27670681e+00 -1.56368423e+00  3.51269037e-01  2.36993122e+00\n",
            "   -1.52318811e+00 -2.11455941e+00 -3.11239939e-02  6.44995987e-01\n",
            "   -8.60882640e-01 -1.08907759e+00 -5.56469858e-01 -2.01824784e+00\n",
            "    1.23225999e+00 -5.13668180e-01  3.22499037e-01  4.90556866e-01\n",
            "   -1.08466379e-01 -1.32243967e+00 -3.22525918e-01 -5.12245476e-01\n",
            "    2.28029862e-02  6.00313723e-01  8.06355417e-01 -1.89911887e-01\n",
            "   -1.24362183e+00 -1.24409842e+00 -4.82711941e-01  7.93745697e-01\n",
            "    7.03088105e-01  1.24396193e+00 -1.41098842e-01  5.06368876e-01]\n",
            "  [-1.24078763e+00  2.11163253e-01 -1.04079939e-01  1.53536868e+00\n",
            "    1.14397061e+00 -1.86497772e+00  4.93627131e-01  1.37187213e-01\n",
            "   -1.13386309e+00  9.13845122e-01  5.48911169e-02  1.03444600e+00\n",
            "    1.57389969e-01  8.97598445e-01  1.50756434e-01  4.27966297e-01\n",
            "   -1.33688256e-01 -1.83411312e+00  4.52713758e-01 -4.39177960e-01\n",
            "    7.25669488e-02  1.28815186e+00 -1.19030511e+00 -1.14671302e+00\n",
            "   -1.10500169e+00  1.02378535e+00  6.80971920e-01  9.28010285e-01\n",
            "    4.65967894e-01 -1.70181584e+00 -8.39350298e-02  1.31792501e-01\n",
            "   -1.26951253e+00 -1.68334997e+00  3.92471969e-01  2.31894255e+00\n",
            "   -1.52654517e+00 -2.25491142e+00  1.58869177e-01  7.42791057e-01\n",
            "   -8.10833991e-01 -8.32327425e-01 -5.70586681e-01 -2.08538246e+00\n",
            "    1.19872499e+00 -6.43332183e-01  3.71049345e-01  4.68686581e-01\n",
            "   -5.49974442e-02 -1.35239542e+00 -4.68300819e-01 -6.57606304e-01\n",
            "   -1.36968613e-01  6.80332482e-01  7.66933203e-01 -2.33892441e-01\n",
            "   -1.16272962e+00 -1.18274748e+00 -3.59141678e-01  8.59767437e-01\n",
            "    7.55345166e-01  1.24407387e+00 -3.37450147e-01  4.46246177e-01]\n",
            "  [-1.13526595e+00  3.03329825e-01 -2.42568806e-01  1.66906524e+00\n",
            "    1.07062972e+00 -1.70841205e+00  5.21308720e-01  1.31660715e-01\n",
            "   -1.03250277e+00  7.85957158e-01  2.52142698e-01  1.03256059e+00\n",
            "   -2.87168343e-02  8.04205716e-01  6.04815595e-02  3.54046255e-01\n",
            "   -2.04349771e-01 -1.78212786e+00  4.98589933e-01 -4.77769703e-01\n",
            "   -1.57662965e-02  1.39486241e+00 -1.27421343e+00 -1.17978799e+00\n",
            "   -1.06741726e+00  9.81101036e-01  7.46873498e-01  9.03569698e-01\n",
            "    5.75354576e-01 -1.69708729e+00  1.56509876e-01  1.64328501e-01\n",
            "   -1.28796637e+00 -1.72045195e+00  4.46148962e-01  2.20617008e+00\n",
            "   -1.60051572e+00 -2.33065748e+00  8.47488716e-02  9.41532433e-01\n",
            "   -8.82948458e-01 -8.57294381e-01 -6.43950045e-01 -2.02874303e+00\n",
            "    1.30379486e+00 -6.59420669e-01  4.20271873e-01  3.41527045e-01\n",
            "    3.73121984e-02 -1.22082114e+00 -5.85131407e-01 -6.20063365e-01\n",
            "   -2.96663523e-01  7.97903359e-01  6.01011992e-01 -1.30089894e-01\n",
            "   -1.35234022e+00 -1.05578768e+00 -3.89407933e-01  7.19891369e-01\n",
            "    7.98543632e-01  1.26927722e+00 -2.79575408e-01  3.58118176e-01]\n",
            "  [-1.21871626e+00  3.57813358e-01 -2.71792978e-01  1.76956713e+00\n",
            "    1.01511109e+00 -1.73963237e+00  6.75257206e-01  3.03297546e-02\n",
            "   -9.04156208e-01  7.84462154e-01  2.33732730e-01  8.85551691e-01\n",
            "    7.11757168e-02  7.62508631e-01  1.39125928e-01  3.64653349e-01\n",
            "   -7.54202902e-02 -1.64953625e+00  5.78767657e-01 -5.33889592e-01\n",
            "    2.24738847e-02  1.49065816e+00 -1.33972454e+00 -1.10901356e+00\n",
            "   -1.16086388e+00  9.86832678e-01  7.64083624e-01  8.23375940e-01\n",
            "    5.00728250e-01 -1.73664165e+00  1.95145085e-01  2.11915389e-01\n",
            "   -1.22351706e+00 -1.64094818e+00  4.35185969e-01  2.17574787e+00\n",
            "   -1.49085414e+00 -2.29519963e+00  8.87169912e-02  1.01715052e+00\n",
            "   -8.55693281e-01 -8.96641552e-01 -6.36269152e-01 -2.07599568e+00\n",
            "    1.23926461e+00 -6.21218383e-01  4.16895270e-01  2.75717378e-01\n",
            "    6.08109124e-02 -1.29925394e+00 -6.15638196e-01 -6.19277298e-01\n",
            "   -3.20850581e-01  7.83373356e-01  5.96346498e-01 -1.08055130e-01\n",
            "   -1.43662524e+00 -1.26315463e+00 -4.23080564e-01  7.73027956e-01\n",
            "    5.55192828e-01  1.26576579e+00 -3.34056914e-01  2.56205171e-01]\n",
            "  [-1.37393665e+00  3.06954861e-01 -2.89852113e-01  1.84101617e+00\n",
            "    1.00325918e+00 -1.68339872e+00  6.41771495e-01 -1.92826334e-02\n",
            "   -9.05959427e-01  8.49319458e-01  7.69694075e-02  7.46761560e-01\n",
            "    1.20299198e-01  6.52820647e-01  1.83986858e-01  1.98801011e-01\n",
            "    3.08785662e-02 -1.61518919e+00  5.92655420e-01 -4.39168036e-01\n",
            "    4.91565056e-02  1.63234758e+00 -1.20882738e+00 -1.25825655e+00\n",
            "   -1.21005189e+00  9.41146255e-01  8.73666704e-01  6.73628151e-01\n",
            "    4.05027956e-01 -1.73783231e+00  2.17507303e-01  1.66651919e-01\n",
            "   -1.15898931e+00 -1.67929173e+00  4.41294253e-01  2.18914509e+00\n",
            "   -1.52766085e+00 -2.18989611e+00  7.80977607e-02  8.80366325e-01\n",
            "   -9.69963729e-01 -8.41341317e-01 -5.09072542e-01 -2.16637444e+00\n",
            "    1.20328701e+00 -7.01371849e-01  4.73805904e-01  1.25988603e-01\n",
            "    1.16118588e-01 -1.29993224e+00 -4.84874338e-01 -5.91241002e-01\n",
            "   -2.69054323e-01  6.50141656e-01  7.03174531e-01 -1.37463048e-01\n",
            "   -1.49263632e+00 -1.29540777e+00 -5.89227080e-01  7.39847660e-01\n",
            "    4.56761390e-01  1.28352678e+00 -3.47600132e-01  2.54138827e-01]\n",
            "  [-1.30451727e+00  3.90180111e-01 -2.71600425e-01  1.91297805e+00\n",
            "    1.13404953e+00 -1.79183531e+00  5.39606571e-01  2.86423322e-03\n",
            "   -9.83475268e-01  8.03184927e-01 -8.87655467e-02  6.83777213e-01\n",
            "    1.70233756e-01  7.26446807e-01 -2.43083630e-02  3.16256642e-01\n",
            "    1.32899161e-03 -1.63376760e+00  6.35474741e-01 -4.49489504e-01\n",
            "   -2.79202964e-02  1.62255442e+00 -1.16279697e+00 -1.31813776e+00\n",
            "   -1.24485731e+00  9.27026868e-01  8.75864744e-01  7.20934987e-01\n",
            "    5.68178236e-01 -1.76892793e+00  2.41465792e-01  2.76938319e-01\n",
            "   -1.03929996e+00 -1.61926043e+00  3.18146259e-01  2.18276954e+00\n",
            "   -1.41995645e+00 -2.14640999e+00  1.46883796e-03  1.01487660e+00\n",
            "   -9.67117786e-01 -7.93942213e-01 -3.97122025e-01 -2.12135744e+00\n",
            "    1.07935929e+00 -5.74235201e-01  4.77809072e-01  2.32461020e-01\n",
            "    1.18988708e-01 -1.30746770e+00 -4.16209996e-01 -6.04603291e-01\n",
            "   -3.05363864e-01  6.96873367e-01  7.62043178e-01 -1.87914893e-01\n",
            "   -1.56704390e+00 -1.36314428e+00 -5.45836270e-01  7.22384274e-01\n",
            "    3.75018746e-01  1.23613167e+00 -2.24511236e-01  2.58414686e-01]]], shape=(1, 10, 64), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FeedForward (with Swiglu)"
      ],
      "metadata": {
        "id": "SekryVil2ZXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FF in Pytorch"
      ],
      "metadata": {
        "id": "MiwlTOE-2gbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward_Torch(nn.Module):\n",
        "  def __init__(self, dim, multiple_of=256, custom_mult=None, dropout=0.0):\n",
        "    super().__init__()\n",
        "    hidden_dim=dim*4\n",
        "    hidden_dim=int(2*hidden_dim/3)\n",
        "    if custom_mult is not None:\n",
        "      hidden_dim=int(dim*custom_mult)\n",
        "    hidden_dim=multiple_of*((hidden_dim+multiple_of-1)//multiple_of)\n",
        "    self.w1=nn.Linear(dim, hidden_dim)\n",
        "    self.w2=nn.Linear(hidden_dim, dim)\n",
        "    self.w3=nn.Linear(dim, hidden_dim)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x=self.w1(x)\n",
        "    swish=F.silu(x)\n",
        "    x_V=self.w3(x)\n",
        "    x=swish*x_V # element wise multiplication\n",
        "    x=self.w2(x)\n",
        "    return self.dropout(x)"
      ],
      "metadata": {
        "id": "jyIlCE102io-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, dim, n_heads, dropout=0.0):\n",
        "    super().__init__()\n",
        "    self.attention=SelfAttentionTorch(dim=dim, n_heads=n_heads)\n",
        "    self.feed_forward=FeedForward_Torch(dim=dim)\n",
        "    self.norm1=RMS_Norm_Torch(dim)\n",
        "    self.norm2=RMS_Norm_Torch(dim)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self, x, start_pos):\n",
        "      x=x+self.dropout(self.attention(self.norm1(x), start_pos=start_pos))\n",
        "      x=x+self.dropout(self.feed_forward(self.norm2(x)))\n",
        "      return x"
      ],
      "metadata": {
        "id": "yj_rOVM4Uj5h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FF in Tensorflow"
      ],
      "metadata": {
        "id": "Qb56acta2d1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward_TF(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim, multiple_of=256, custom_mult=None, dropout=0.0):\n",
        "    super().__init__()\n",
        "    hidden_dim=dim*4\n",
        "    hidden_dim=int(2*hidden_dim/3)\n",
        "    if custom_mult is not None:\n",
        "      hidden_dim=int(dim*custom_mult)\n",
        "    hidden_dim=multiple_of*((hidden_dim+multiple_of-1)//multiple_of)\n",
        "\n",
        "    self.w1=tf.keras.layers.Dense(hidden_dim, use_bias=False)\n",
        "    self.w2=tf.keras.layers.Dense(dim, use_bias=False) # Final layer\n",
        "    self.w3=tf.keras.layers.Dense(hidden_dim, use_bias=False)\n",
        "    self.dropout=tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def call(self,x):\n",
        "    x=self.w1(x)\n",
        "    swish=tf.nn.silu(x)\n",
        "    x_V=self.w3(x)\n",
        "    x=swish*x_V # element wise multiplication\n",
        "    x=self.w2(x)\n",
        "    return self.dropout(x)\n"
      ],
      "metadata": {
        "id": "VS2Zn4h92jQF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, dim, n_heads, max_seq_len, batch_size, dropout=0.0, use_cache=False):\n",
        "    super().__init__()\n",
        "    self.attention=SelfAttentionTF(dim=dim, n_heads=n_heads, max_seq_len=max_seq_len, max_batch_size=batch_size, use_cache=use_cache)\n",
        "    self.feed_forward=FeedForward_TF(dim=dim)\n",
        "    self.norm1=RMS_Norm_TF(dim)\n",
        "    self.norm2=RMS_Norm_TF(dim)\n",
        "    self.dropout=tf.keras.layers.Dropout(dropout)\n",
        "  def call(self, x, start_pos):\n",
        "    x=x+self.dropout(self.attention(self.norm1(x), start_pos=start_pos))\n",
        "    x=x+self.dropout(self.feed_forward(self.norm2(x)))\n",
        "    return x"
      ],
      "metadata": {
        "id": "6PCW-x9tShdj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test Encoder\n",
        "x=tf.ones((1,10,128))\n",
        "enc=EncoderLayer(128, 16, 10, 32)\n",
        "enc(x, start_pos=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aC8e-QyDJ2G",
        "outputId": "192d4d0d-3aaf-4465-c6d0-20d083f5aa70"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10, 128), dtype=float32, numpy=\n",
              "array([[[ 0.1210289 ,  0.4260625 ,  0.534716  , ...,  0.2922683 ,\n",
              "          0.7803144 , -0.55545443],\n",
              "        [ 0.12102893,  0.4260623 ,  0.534716  , ...,  0.2922682 ,\n",
              "          0.78031427, -0.55545443],\n",
              "        [ 0.1210289 ,  0.4260621 ,  0.53471595, ...,  0.29226857,\n",
              "          0.780314  , -0.55545473],\n",
              "        ...,\n",
              "        [ 0.12102884,  0.42606243,  0.5347161 , ...,  0.29226837,\n",
              "          0.78031456, -0.5554542 ],\n",
              "        [ 0.12102899,  0.42606243,  0.5347157 , ...,  0.29226813,\n",
              "          0.7803142 , -0.5554545 ],\n",
              "        [ 0.12102884,  0.4260621 ,  0.5347161 , ...,  0.29226822,\n",
              "          0.78031427, -0.55545455]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final showdown"
      ],
      "metadata": {
        "id": "bLg87CFS0gUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder block"
      ],
      "metadata": {
        "id": "EOt9GDErSnK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, dim, n_heads, n_layers, max_seq_len, batch_size, vocab_size, eps=1e-6, dropout=0.0, use_cache=False):\n",
        "    super().__init__()\n",
        "    assert vocab_size!=-1, \"Vocab size must be set\"\n",
        "    self.token_embeddings=tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=dim)\n",
        "    self.encoder_layers=[EncoderLayer(dim=dim, n_heads=n_heads, max_seq_len=max_seq_len, batch_size=batch_size, dropout=dropout, use_cache=use_cache) for _ in range(n_layers)]\n",
        "    self.norm=RMS_Norm_TF(dim, eps=eps)\n",
        "    self.final_layer=tf.keras.layers.Dense(vocab_size, use_bias=False)\n",
        "\n",
        "  def call(self, x):\n",
        "    batch_size, seq_len=x.shape\n",
        "    x=self.token_embeddings(x)\n",
        "    for i in range(len(self.encoder_layers)):\n",
        "      x=self.encoder_layers[i](x, start_pos=0)\n",
        "    x=self.norm(x)\n",
        "    logits=self.final_layer(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Xh7RC-wmSqOc"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test transformer\n",
        "x=tf.random.uniform((1,10), minval=0, maxval=100, dtype=tf.int32)\n",
        "transformer=Transformer(dim=64, n_heads=16, n_layers=1, max_seq_len=10, batch_size=1, vocab_size=100, use_cache=True)\n",
        "transformer(x)\n"
      ],
      "metadata": {
        "id": "MPjcGOli1LNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881ca6f1-f6ca-4e4f-bd9b-3fc8bcf28465"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10, 100), dtype=float32, numpy=\n",
              "array([[[-1.11166120e+00,  6.41244769e-01, -4.09252465e-01,\n",
              "         -3.30242842e-01,  5.90650141e-01, -3.13680589e-01,\n",
              "         -1.76802315e-02,  5.72568297e-01,  2.21795782e-01,\n",
              "          1.16955495e+00, -1.19740689e+00,  1.27444410e+00,\n",
              "         -9.74639654e-01, -4.57591921e-01,  6.84647739e-01,\n",
              "          5.06904006e-01, -1.72054362e+00,  1.19893670e+00,\n",
              "          8.80591035e-01,  6.67242527e-01, -3.37746799e-01,\n",
              "         -5.29994726e-01,  1.05022812e+00,  1.85814071e+00,\n",
              "          2.47316048e-01, -1.85167694e+00,  7.07224727e-01,\n",
              "          1.16831601e+00, -2.14823937e+00, -1.51316512e+00,\n",
              "          1.35644639e+00, -2.56015867e-01,  9.17008758e-01,\n",
              "          1.12232864e-01,  5.42326927e-01, -9.00227189e-01,\n",
              "          3.83093096e-02,  1.05395103e+00,  1.39789328e-01,\n",
              "         -1.26404274e+00, -1.70159554e+00,  5.64920962e-01,\n",
              "         -1.44117093e-02,  5.58448970e-01, -1.42117047e+00,\n",
              "         -1.72615349e-02, -1.07714760e+00,  1.02153289e+00,\n",
              "          1.61983061e+00,  1.59366798e+00,  9.44884956e-01,\n",
              "          5.46098173e-01,  8.59319448e-01, -9.97353971e-01,\n",
              "          3.02327663e-01, -1.00861824e+00,  2.77680755e-01,\n",
              "         -1.08967865e+00,  6.38934612e-01, -2.78549761e-01,\n",
              "         -9.33916986e-01,  2.68980324e-01, -7.16079354e-01,\n",
              "          1.37151432e+00,  2.83006549e-01, -1.45569384e+00,\n",
              "         -1.50328481e+00,  1.32593006e-01, -1.30354881e+00,\n",
              "          1.07872713e+00,  2.68454611e-01,  2.66484559e-01,\n",
              "          3.08031023e-01,  3.71590048e-01, -4.03246373e-01,\n",
              "          7.23125815e-01, -2.58785099e-01, -1.45692384e+00,\n",
              "         -3.89221042e-01, -9.12390947e-01, -1.03721268e-01,\n",
              "          1.14872050e+00,  3.61651957e-01,  1.47740650e+00,\n",
              "          2.62771994e-01,  1.14839956e-01, -9.72743273e-01,\n",
              "         -1.28359431e-02, -1.24313140e+00, -2.36436009e-01,\n",
              "          1.15326691e+00,  5.38143933e-01, -2.50243582e-02,\n",
              "         -2.52325118e-01, -6.21349774e-02,  5.30802786e-01,\n",
              "          1.45496368e+00, -1.11862481e-01,  6.42305732e-01,\n",
              "          5.79265833e-01],\n",
              "        [-1.12452149e+00,  1.10336673e+00,  1.30127668e+00,\n",
              "          5.73410809e-01,  1.58060029e-01,  2.29623511e-01,\n",
              "         -1.16394854e+00, -3.07417661e-01, -4.08750921e-01,\n",
              "          1.23229015e+00,  1.63898572e-01, -7.82962799e-01,\n",
              "          1.18732683e-01,  1.20346069e+00,  1.61566103e+00,\n",
              "         -3.59660573e-02, -1.81561363e+00,  1.29004884e+00,\n",
              "         -3.04278553e-01,  3.79867613e-01, -2.56248891e-01,\n",
              "         -1.51936933e-01,  2.11684227e+00,  6.11171901e-01,\n",
              "         -4.62137610e-01, -1.38475132e+00,  1.62288618e+00,\n",
              "         -3.69432747e-01, -7.21135497e-01, -6.40309930e-01,\n",
              "          2.16011047e+00,  3.14731263e-02, -1.14910340e+00,\n",
              "          6.19470716e-01, -4.64689970e-01,  1.23472691e+00,\n",
              "          1.40663505e+00,  1.27706587e+00, -7.86518872e-01,\n",
              "          2.24138066e-01, -1.83920249e-01,  1.31142604e+00,\n",
              "          1.17108333e+00, -1.20097376e-01, -1.13827467e+00,\n",
              "         -3.39051545e-01, -4.64995466e-02,  1.25630057e+00,\n",
              "          6.25849068e-02,  1.40309167e+00,  8.46694529e-01,\n",
              "         -1.71253204e+00,  1.09408748e+00,  1.70460135e-01,\n",
              "          3.22778285e-01, -9.56491102e-03,  1.61777124e-01,\n",
              "          3.93422358e-02,  5.72068453e-01, -3.57614398e-01,\n",
              "          1.90919369e-01, -1.49882540e-01, -4.10399646e-01,\n",
              "          4.18671012e-01, -1.61158407e+00,  4.51860242e-02,\n",
              "         -1.19341457e+00,  1.77512243e-01,  6.59950916e-03,\n",
              "         -1.23914585e-01, -1.15192607e-01,  1.29198658e+00,\n",
              "         -6.91746712e-01,  1.04113154e-01, -3.64031136e-01,\n",
              "         -1.64504275e-01,  1.14632450e-01, -1.53308913e-01,\n",
              "         -3.57156426e-01, -1.03517920e-01, -1.29542649e+00,\n",
              "          1.99723542e+00, -8.57954562e-01, -5.90708740e-02,\n",
              "         -2.18665943e-01,  1.12240888e-01, -3.25698078e-01,\n",
              "          1.70677945e-01, -1.39617884e+00, -7.69278049e-01,\n",
              "         -3.76684427e-01, -2.62720495e-01,  4.15759444e-01,\n",
              "         -3.65921825e-01, -8.39249313e-01,  9.10493314e-01,\n",
              "          1.20648265e+00, -3.20817798e-01,  4.61878300e-01,\n",
              "         -5.17318130e-01],\n",
              "        [-1.15491986e+00,  2.36166883e+00,  4.78808254e-01,\n",
              "          4.78072494e-01,  2.40406483e-01, -8.24641168e-01,\n",
              "          5.17156601e-01,  1.28413856e-01,  1.08559906e-01,\n",
              "          1.31126940e+00,  7.02987075e-01,  2.73723394e-01,\n",
              "         -3.14432867e-02, -2.18406126e-01,  9.86694276e-01,\n",
              "         -8.92430067e-01, -4.37161535e-01, -1.19427681e+00,\n",
              "          5.17533958e-01,  1.42870200e+00, -1.12852836e+00,\n",
              "         -2.13282406e-01,  1.15155113e+00,  7.40483165e-01,\n",
              "          7.50249088e-01, -1.00999570e+00,  1.58169448e+00,\n",
              "          8.53519976e-01, -6.47211134e-01, -7.12434351e-01,\n",
              "          1.91942549e+00,  1.17074668e+00,  1.10605085e+00,\n",
              "          2.41364434e-01, -8.97376239e-01, -2.67962664e-01,\n",
              "          3.98583747e-02,  1.33596218e+00, -8.84498537e-01,\n",
              "         -1.78236559e-01, -1.95730340e+00, -4.56934184e-01,\n",
              "          2.50452071e-01,  1.72025895e+00, -2.65446997e+00,\n",
              "          9.27772045e-01,  4.48487908e-01,  2.36140922e-01,\n",
              "          9.40856814e-01,  2.09597802e+00,  8.68195832e-01,\n",
              "         -1.22851562e+00,  1.51612687e+00,  6.04809105e-01,\n",
              "          4.65192974e-01, -1.64453065e+00,  7.77607262e-01,\n",
              "         -6.61611855e-01,  7.22454488e-01,  2.82641221e-02,\n",
              "         -1.50305009e+00, -7.40680933e-01, -1.83251083e-01,\n",
              "          8.77656877e-01, -5.02110481e-01, -3.59764278e-01,\n",
              "         -7.81195879e-01,  1.19135812e-01,  3.28067243e-01,\n",
              "          1.09298527e+00, -5.46174526e-01, -3.39426816e-01,\n",
              "         -8.72420251e-01, -4.62916940e-01,  3.54979336e-01,\n",
              "          5.29362142e-01, -3.10263336e-01, -1.34287262e+00,\n",
              "         -6.54658794e-01, -1.11415911e+00, -8.05665832e-03,\n",
              "          1.16700041e+00,  1.04254985e+00,  6.32240832e-01,\n",
              "         -2.63839275e-01, -7.66295254e-01, -2.20130301e+00,\n",
              "          1.25103474e-01, -1.10823190e+00,  1.25402641e+00,\n",
              "          2.39799953e+00, -8.65005851e-02,  4.69489694e-01,\n",
              "          1.05563748e+00, -1.56174791e+00, -2.36055683e-02,\n",
              "          3.80222797e-01, -1.30667722e+00,  1.52758837e+00,\n",
              "         -2.35729381e-01],\n",
              "        [-1.04987526e+00,  4.16634411e-01,  7.60971427e-01,\n",
              "          4.62172568e-01, -1.22820184e-01,  3.95403683e-01,\n",
              "         -2.08930463e-01,  8.63230526e-01,  3.42403978e-01,\n",
              "          5.71392059e-01, -6.70294538e-02, -4.79944885e-01,\n",
              "         -1.19254923e+00, -1.68791056e-01, -4.36891094e-02,\n",
              "          9.50656652e-01, -8.82820264e-02,  7.27151453e-01,\n",
              "          7.25373685e-01,  1.16758786e-01, -9.52041184e-04,\n",
              "          2.60022968e-01,  1.57528117e-01,  1.51102924e+00,\n",
              "         -1.44191098e+00, -3.77723187e-01,  2.76252651e+00,\n",
              "         -2.63017893e-01, -7.37926960e-02, -3.51872206e-01,\n",
              "          1.23744476e+00, -1.42108768e-01,  3.13860118e-01,\n",
              "         -1.36911798e+00, -4.48767543e-01,  8.73699248e-01,\n",
              "          2.10569397e-01,  1.36053145e+00,  8.01895022e-01,\n",
              "         -2.87934303e-01,  3.43179375e-01,  2.74275541e-01,\n",
              "          7.43137747e-02,  9.63292181e-01, -1.68884599e+00,\n",
              "         -1.56257224e+00, -5.23146391e-01,  4.84980434e-01,\n",
              "          5.85479140e-01,  6.08432114e-01, -6.19677484e-01,\n",
              "         -8.94416094e-01, -1.55091822e-01, -1.51810467e-01,\n",
              "          3.11541468e-01, -4.50851083e-01,  6.88652415e-03,\n",
              "          5.68777740e-01,  2.05996609e+00,  2.80799299e-01,\n",
              "         -1.83479548e+00,  2.70383060e-01,  3.49489212e-01,\n",
              "         -6.50979102e-01, -1.29188061e+00, -4.85116392e-01,\n",
              "         -1.82212496e+00, -2.50605911e-01,  1.62646818e+00,\n",
              "         -1.68074697e-01,  2.67240498e-02,  2.81339079e-01,\n",
              "         -2.78019965e-01,  1.78539485e-01, -1.09806228e+00,\n",
              "         -1.61600053e-01,  5.19358218e-01, -2.34547511e-01,\n",
              "         -1.22826301e-01, -1.03666246e+00, -1.55911386e+00,\n",
              "          4.75385785e-01, -2.96876580e-01,  6.84313118e-01,\n",
              "         -3.30710679e-01, -3.82988185e-01, -4.98472601e-01,\n",
              "         -6.54252410e-01, -6.18299663e-01, -1.39587164e+00,\n",
              "         -4.11818504e-01,  6.45586371e-01,  5.17383218e-01,\n",
              "         -5.69309294e-01, -1.60907578e+00, -2.05565736e-01,\n",
              "          1.22271550e+00, -1.49145615e+00,  2.07208014e+00,\n",
              "          1.82411805e-01],\n",
              "        [-1.31506610e+00,  2.07066965e+00, -3.55775803e-01,\n",
              "          3.78602415e-01, -2.17452139e-01,  4.41555321e-01,\n",
              "         -2.62230694e-01, -1.09510708e+00,  4.41936702e-01,\n",
              "          1.28734434e+00, -7.68461645e-01, -6.63570404e-01,\n",
              "          1.30294144e+00, -7.82583773e-01,  8.80849361e-01,\n",
              "          7.00830758e-01, -1.19210410e+00,  1.07276070e+00,\n",
              "         -7.46423662e-01,  1.01938283e+00, -1.91860899e-01,\n",
              "          1.10091138e+00,  2.04751110e+00,  1.11585009e+00,\n",
              "          2.20065713e-02, -8.09475303e-01,  1.27874768e+00,\n",
              "          5.65763056e-01, -1.37590361e+00, -8.08263600e-01,\n",
              "          1.67386949e+00,  7.83506930e-01,  1.01156667e-01,\n",
              "         -7.49541759e-01, -6.89557076e-01,  2.50276476e-02,\n",
              "         -1.61428362e-01,  4.11668032e-01, -1.96565616e+00,\n",
              "          3.00587445e-01, -2.51900733e-01,  1.03248334e+00,\n",
              "         -6.85157537e-01,  2.61232138e-01,  1.36396438e-01,\n",
              "          1.01310956e+00,  2.98031807e-01,  1.37985945e-01,\n",
              "          2.18706441e+00,  7.11216331e-01, -6.35388345e-02,\n",
              "         -1.05170119e+00,  2.31723118e+00,  2.78289527e-01,\n",
              "          5.89297295e-01, -2.02506208e+00,  1.33441699e+00,\n",
              "         -1.78800821e-01, -9.91086960e-01,  5.52499652e-01,\n",
              "          1.93751228e+00, -1.09536004e+00, -2.62236506e-01,\n",
              "          1.85917124e-01, -4.96506602e-01, -6.41288087e-02,\n",
              "         -7.36388624e-01,  8.80964220e-01, -7.78619766e-01,\n",
              "          1.47811794e+00,  1.38192654e-01, -5.34479141e-01,\n",
              "          9.56814945e-01,  8.39660883e-01,  4.23924208e-01,\n",
              "         -9.91538644e-01,  3.94544363e-01, -4.28572327e-01,\n",
              "          9.52383518e-01, -7.92925954e-01, -2.97380108e-02,\n",
              "          9.41891432e-01, -1.31668568e+00,  4.32982206e-01,\n",
              "         -3.01658779e-01,  1.86881638e+00, -1.25582826e+00,\n",
              "         -1.01288581e+00,  7.83577919e-01,  6.78671002e-01,\n",
              "          1.56764734e+00,  2.13779677e-02, -3.97097349e-01,\n",
              "          2.37023664e+00,  1.50020218e+00,  3.92035842e-01,\n",
              "          2.39050651e+00,  1.21149707e+00, -9.26716566e-01,\n",
              "         -1.46197692e-01],\n",
              "        [-1.91884887e+00,  1.16589105e+00,  1.62445402e+00,\n",
              "          1.40599936e-01, -5.72997183e-02, -4.91049200e-01,\n",
              "         -1.01747727e+00,  7.07045794e-01,  2.99219042e-01,\n",
              "         -9.68225077e-02,  4.35400277e-01, -1.18257844e+00,\n",
              "          2.33726516e-01,  6.96321726e-01,  1.30717993e+00,\n",
              "         -3.63072187e-01, -1.51999855e+00,  7.07946956e-01,\n",
              "         -9.65264797e-01,  2.00248018e-01,  1.01330173e+00,\n",
              "         -1.29035532e+00,  9.37436402e-01, -4.16081190e-01,\n",
              "         -4.66697961e-01, -5.72739482e-01,  2.33850169e+00,\n",
              "         -2.90573508e-01,  2.78536767e-01, -4.45771694e-01,\n",
              "          1.61917984e+00, -1.06630194e+00, -4.98670459e-01,\n",
              "          3.52336705e-01, -7.74416924e-01,  1.21130872e+00,\n",
              "         -8.85283872e-02,  7.54646242e-01, -1.01301360e+00,\n",
              "         -6.56064212e-01, -1.25730777e+00,  7.36001611e-01,\n",
              "          6.97128832e-01,  1.36863187e-01, -3.54528964e-01,\n",
              "         -9.10185814e-01, -4.62879509e-01,  1.38342440e-01,\n",
              "         -6.37245595e-01,  9.35688198e-01,  9.40213561e-01,\n",
              "         -1.30037510e+00,  1.40214062e+00,  1.21787953e+00,\n",
              "         -1.04188251e+00, -5.33118069e-01, -7.18591511e-01,\n",
              "          3.79252853e-03, -4.44777191e-01, -1.07176147e-01,\n",
              "         -8.21969986e-01,  5.34151196e-01,  7.26450861e-01,\n",
              "          5.69205821e-01, -1.03161418e+00,  1.45088106e-01,\n",
              "          3.28605950e-01,  1.24845453e-01, -4.47595529e-02,\n",
              "         -2.40832809e-02, -3.37277770e-01,  9.47259188e-01,\n",
              "         -6.56498909e-01, -4.47068036e-01,  3.52895886e-01,\n",
              "          5.69180965e-01,  5.75263426e-02,  2.93102711e-01,\n",
              "         -6.00214481e-01,  8.61169279e-01, -7.80999541e-01,\n",
              "          1.51681292e+00, -2.99453855e-01, -1.00835299e+00,\n",
              "          1.01197803e+00, -1.02261889e+00, -7.47407198e-01,\n",
              "          3.39706987e-01, -1.66171741e+00, -4.01949227e-01,\n",
              "         -1.92860976e-01, -6.37572408e-01, -1.38904676e-01,\n",
              "          2.66602039e-01, -1.01415908e+00,  5.79473376e-01,\n",
              "          6.57002926e-01, -7.10089028e-01,  1.96944618e+00,\n",
              "         -5.93650281e-01],\n",
              "        [ 4.18543555e-02,  8.63251388e-01,  1.76278651e-01,\n",
              "          5.58793128e-01,  1.19104826e+00, -2.41933680e+00,\n",
              "         -3.07193339e-01,  1.06238878e+00, -6.34909928e-01,\n",
              "          3.59338641e-01,  7.74844140e-02,  5.97451866e-01,\n",
              "         -5.69954276e-01,  1.24939251e+00, -1.72026083e-01,\n",
              "         -7.97505438e-01, -1.62866667e-01, -7.91268945e-01,\n",
              "         -2.94626266e-01,  7.72226393e-01, -5.36773920e-01,\n",
              "         -6.72710538e-01,  1.24042857e+00,  1.08808386e+00,\n",
              "         -6.36317611e-01, -3.81979756e-02,  6.00912213e-01,\n",
              "         -1.53775424e-01,  6.42919242e-02, -8.36011350e-01,\n",
              "          1.55157030e+00,  1.14413567e-01,  3.56063396e-01,\n",
              "         -1.25386310e+00, -4.12340343e-01, -1.15964973e+00,\n",
              "          6.14402108e-02,  1.04639888e+00,  8.01915675e-02,\n",
              "          5.10132372e-01, -2.04998279e+00,  2.56357670e-01,\n",
              "          1.67742300e+00,  4.95251507e-01, -5.65800190e-01,\n",
              "         -3.99250209e-01,  5.74128211e-01,  8.24834883e-01,\n",
              "          3.71916175e-01,  1.02774715e+00,  7.44182587e-01,\n",
              "          3.05497319e-01,  8.55971694e-01,  9.67084587e-01,\n",
              "          1.02082980e+00,  2.38667339e-01, -1.13015890e+00,\n",
              "         -2.39695206e-01,  5.71654737e-01,  4.94999349e-01,\n",
              "         -1.25321686e+00,  7.00673401e-01, -3.04085195e-01,\n",
              "          1.89903355e+00, -8.67933810e-01, -4.93244410e-01,\n",
              "         -1.36186206e+00,  5.70914745e-01,  2.57178187e-01,\n",
              "          1.15066516e+00,  6.95677400e-01,  5.87055326e-01,\n",
              "         -2.80002445e-01,  1.81090206e-01,  4.41837311e-02,\n",
              "          1.74242079e+00, -6.76653862e-01, -6.61516115e-02,\n",
              "         -3.55366394e-02, -2.50341028e-01,  2.71178037e-01,\n",
              "          1.33454025e+00,  7.30459020e-02,  7.52620399e-01,\n",
              "          9.09511089e-01, -1.85170714e-02, -3.15632552e-01,\n",
              "          3.98206897e-02, -6.45025313e-01, -1.42231643e-01,\n",
              "          1.20359814e+00,  3.52093697e-01,  2.73548305e-01,\n",
              "          1.20740116e+00, -1.92940295e+00, -4.60189402e-01,\n",
              "          1.92307457e-01, -6.70438707e-01,  2.48946381e+00,\n",
              "          3.42629492e-01],\n",
              "        [-1.35828471e+00,  1.15404856e+00, -6.58107623e-02,\n",
              "          1.20056465e-01,  6.61110729e-02,  1.48812640e+00,\n",
              "          1.46348619e+00, -9.10335958e-01,  7.92120278e-01,\n",
              "          4.36470091e-01,  5.59248090e-01, -8.92806649e-01,\n",
              "         -9.92764384e-02, -5.50792873e-01,  6.93804994e-02,\n",
              "         -3.57502997e-02, -4.40282881e-01, -6.46256566e-01,\n",
              "         -8.95938694e-01,  1.57348740e+00,  2.51295477e-01,\n",
              "          7.39214182e-01,  3.14725675e-02,  1.42760789e+00,\n",
              "         -1.20056176e+00, -1.42077172e+00,  2.03734541e+00,\n",
              "          6.28160954e-01, -9.88676429e-01, -6.33474946e-01,\n",
              "          1.11994576e+00,  2.09293351e-01, -1.80778056e-01,\n",
              "         -1.42951536e+00, -3.23482454e-01, -9.55746651e-01,\n",
              "         -3.13981026e-01, -7.58981228e-01, -7.54175305e-01,\n",
              "          5.26072122e-02, -1.09116161e+00,  2.76089609e-02,\n",
              "         -1.05852449e+00,  6.10745907e-01, -6.58477783e-01,\n",
              "          2.61945963e-01, -8.26823294e-01,  1.33142161e+00,\n",
              "          1.06205463e+00,  1.79664999e-01, -6.65945470e-01,\n",
              "         -3.07695210e-01,  1.23403060e+00,  6.99241936e-01,\n",
              "          7.27735981e-02, -9.77851748e-01,  9.08398330e-01,\n",
              "         -8.91522020e-02, -1.27685988e+00,  1.16828477e+00,\n",
              "          3.57079476e-01, -2.30052304e+00, -5.34357250e-01,\n",
              "         -7.66960800e-01, -5.49230203e-02, -2.45090032e+00,\n",
              "         -9.25022662e-01,  4.41219568e-01,  3.78907710e-01,\n",
              "          6.48704290e-01, -5.01535714e-01,  3.40839922e-01,\n",
              "          9.44506526e-01,  5.16702831e-01, -4.62690264e-01,\n",
              "         -5.79289496e-01,  7.96658173e-02, -1.12760756e-02,\n",
              "          8.37934315e-01,  7.00001279e-03, -1.45677045e-01,\n",
              "         -2.95114815e-01, -1.68324113e-01, -4.44828123e-01,\n",
              "         -2.59599775e-01, -7.29378104e-01, -1.68744946e+00,\n",
              "         -8.68656337e-01, -6.26807570e-01, -1.29531658e+00,\n",
              "          9.76872027e-01, -5.98397851e-01, -7.54352868e-01,\n",
              "          2.02636337e+00, -9.70527291e-01,  1.25953829e+00,\n",
              "          2.82681614e-01, -2.46257484e-01,  7.98739672e-01,\n",
              "         -5.29897928e-01],\n",
              "        [-9.44383502e-01,  1.86819649e+00,  5.96172035e-01,\n",
              "          4.99543220e-01,  4.37441049e-04,  4.79853511e-01,\n",
              "          2.04217124e+00,  1.31691647e+00, -7.74953067e-01,\n",
              "          1.72623014e+00, -1.03307462e+00, -1.23124018e-01,\n",
              "          6.56097591e-01, -4.88366246e-01,  7.75101960e-01,\n",
              "          1.02687013e+00, -1.25879300e+00,  2.34572425e-01,\n",
              "         -1.51476121e+00,  4.92197365e-01, -1.17173994e+00,\n",
              "          2.53052115e-01,  5.23334861e-01,  7.00146198e-01,\n",
              "         -4.19254214e-01, -1.52287078e+00,  7.72724509e-01,\n",
              "         -1.80787802e-01,  3.71330351e-01,  6.25027776e-01,\n",
              "         -1.23456372e-02,  4.00002033e-01, -1.16682881e-02,\n",
              "         -9.44579542e-01, -4.03357506e-01,  6.55193269e-01,\n",
              "          2.51731545e-01,  1.85125098e-01, -2.50017554e-01,\n",
              "         -1.59255534e-01, -1.20834261e-01,  4.94559616e-01,\n",
              "         -2.89359957e-01,  1.10926247e+00, -1.26609397e+00,\n",
              "         -2.51028538e-01,  4.14102137e-01,  5.15004456e-01,\n",
              "          2.46823883e+00,  7.60297120e-01,  4.68010038e-01,\n",
              "         -7.87260413e-01,  1.19948184e+00,  1.76706016e+00,\n",
              "          1.28201640e+00, -4.66886312e-01,  1.51299572e+00,\n",
              "          1.18561387e+00,  2.13226512e-01,  3.15365851e-01,\n",
              "         -7.23575890e-01, -6.15454257e-01, -3.26965392e-01,\n",
              "          2.29121208e-01, -9.50755596e-01,  2.77001560e-01,\n",
              "         -7.63119936e-01,  7.95661569e-01, -1.20347448e-01,\n",
              "          7.40684450e-01, -3.23776552e-03, -7.94926167e-01,\n",
              "         -9.85247199e-04,  2.42129162e-01, -1.42340970e+00,\n",
              "         -3.03709209e-01, -7.27606177e-01,  2.39188254e-01,\n",
              "          1.91425592e-01, -1.41706967e+00, -2.73934267e-02,\n",
              "          2.15921903e+00, -6.83659971e-01, -3.10617089e-01,\n",
              "          3.06492150e-01,  1.06981230e+00, -1.49262846e+00,\n",
              "         -5.88080510e-02, -2.58550376e-01, -1.59498942e+00,\n",
              "          9.45314541e-02,  9.80778217e-01,  2.19625741e-01,\n",
              "          6.28836870e-01, -2.67024674e-02,  2.55337298e-01,\n",
              "          1.72810435e+00,  6.16114914e-01,  2.68139780e-01,\n",
              "          7.09714890e-01],\n",
              "        [-1.87849069e+00,  2.68469989e-01, -1.34350613e-01,\n",
              "          2.62206614e-01, -4.35109079e-01, -6.24843836e-01,\n",
              "          9.09761369e-01,  9.90969121e-01,  1.17705750e+00,\n",
              "         -6.26706958e-01,  1.14648032e+00, -8.96705568e-01,\n",
              "          1.23017989e-01, -6.69948384e-02,  8.16782475e-01,\n",
              "          1.27688721e-01, -7.73931205e-01,  6.89092994e-01,\n",
              "         -1.08260244e-01,  7.11156607e-01,  3.01100373e-01,\n",
              "         -9.19802427e-01, -6.82798326e-01,  5.93825102e-01,\n",
              "         -9.51017961e-02, -5.57733417e-01,  2.10126066e+00,\n",
              "          6.43607080e-01,  6.42314553e-01, -1.01086266e-01,\n",
              "         -6.91101551e-01, -5.98230898e-01,  2.41393715e-01,\n",
              "         -1.68421221e+00, -3.17749679e-01,  2.08672333e+00,\n",
              "          9.95236486e-02,  3.00520122e-01,  5.84633112e-01,\n",
              "         -1.32261074e+00, -3.09456959e-02,  1.11258900e+00,\n",
              "         -1.09114015e+00, -3.20021838e-01, -4.87881660e-01,\n",
              "         -7.27471292e-01,  2.63656944e-01,  4.70027983e-01,\n",
              "          1.78184295e+00, -6.68363690e-01,  3.22888970e-01,\n",
              "          9.64377671e-02,  1.28486663e-01,  3.59201610e-01,\n",
              "         -5.48451066e-01, -1.47430271e-01,  1.49044085e+00,\n",
              "          6.24580085e-01,  9.64069843e-01, -5.38136840e-01,\n",
              "         -1.67200100e+00,  1.18054485e+00,  7.55287111e-01,\n",
              "         -9.12326038e-01,  2.66655207e-01, -5.57072341e-01,\n",
              "         -9.46914852e-02,  1.82591081e-01,  8.57469440e-01,\n",
              "         -3.45621586e-01, -1.97122425e-01, -7.67065167e-01,\n",
              "          3.67431864e-02,  4.20688719e-01, -8.79787654e-03,\n",
              "         -6.10219657e-01,  1.21682656e+00,  7.90373906e-02,\n",
              "         -4.25815791e-01, -1.06938434e+00, -8.80212367e-01,\n",
              "          7.60407865e-01, -5.46061397e-01, -4.32857662e-01,\n",
              "          8.35957766e-01, -2.76224256e-01, -1.19807386e+00,\n",
              "         -4.80961651e-01, -3.43106270e-01, -1.41852510e+00,\n",
              "          1.40162468e-01, -1.51837802e+00, -7.76347458e-01,\n",
              "         -1.97825328e-01,  1.05636701e-01, -1.22395016e-01,\n",
              "         -3.11571926e-01, -1.29788280e+00,  1.24024022e+00,\n",
              "          2.48876765e-01]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}